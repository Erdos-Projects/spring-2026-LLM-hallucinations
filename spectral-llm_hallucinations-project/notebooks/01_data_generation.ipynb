{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044669a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# --- Environment Setup and Key Loading ---\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "from google.colab import userdata, drive\n",
    "from huggingface_hub import login\n",
    "from google.colab import runtime\n",
    "\n",
    "github_token = userdata.get('GITHUB_TOKEN')\n",
    "\n",
    "# Info regarding our repo\n",
    "repo_owner = \"Erdos-Projects\" \n",
    "branch_name = \"hw/eigenvalues-extraction\"\n",
    "repo_name = \"spring-2026-LLM-hallucinations\"\n",
    "repo_url = f\"https://{github_token}@github.com/{repo_owner}/{repo_name}.git\"\n",
    "\n",
    "# Clone the github repository into the Colab VM\n",
    "!rm -rf /content/repo\n",
    "!git clone -b {branch_name} {repo_url} /content/repo\n",
    "\n",
    "!pip install -q -r /content/repo/requirements.txt\n",
    "sys.path.append('/content/repo/spectral-llm_hallucinations-project')\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Setup OpenAI and HuggingFace tokens\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY').strip()\n",
    "login(token=userdata.get('HF_TOKEN'))\n",
    "\n",
    "from spectral_detection import data_generation\n",
    "from spectral_detection.data import loaders\n",
    "\n",
    "print(\"Initialization was successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c956c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# --- Generate multiple samples for TruthfulQA/TriviaQA\n",
    "\n",
    "pipeline = data_generation.Pipeline()\n",
    "truthfulqa_data = loaders.load_truthfulqa(sample_size=500)\n",
    "#triviaqa_data = loaders.load_triviaqa(sample_size=500)\n",
    "\n",
    "# Executes and saves to /content/drive/MyDrive/spectral_pipeline/truthfulqa_t1.0_n20.jsonl\n",
    "#                       /content/drive/MyDrive/spectral_pipeline/triviaqa_t1.0_n20.jsonl\n",
    "\n",
    "pipeline.generate_dataset(\n",
    "    data_list=truthfulqa_data, # triviaqa_data\n",
    "    answers_per_prompt=20,\n",
    "    dataset_name=\"truthfulqa\", # triviaqa\n",
    "    temperature=1.0,\n",
    "    overwrite=True \n",
    ")\n",
    "\n",
    "print(\"Dataset successfully generated\")\n",
    "\n",
    "# --- Judging 20 samples per prompt ---\n",
    "judge = data_generation.LLMJudge(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "target_path = \"/content/drive/MyDrive/spectral_pipeline/truthfulqa_t1.0_n20.jsonl\" # \"/content/drive/MyDrive/spectral_pipeline/triviaqa_t1.0_n20.jsonl\"\n",
    "judge.evaluate_file(target_path)\n",
    "\n",
    "# Build dataframe\n",
    "df = pd.read_json(target_path, lines=True)\n",
    "\n",
    "# Columns for inspection\n",
    "display(df[['id', 'sample_num', 'question', 'correctness', 'domain', 'correctness_score']])\n",
    "\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# --- Evaluation of MMLU 20 samples (Semantic entropy) ---\n",
    "\n",
    "pipeline = data_generation.Pipeline()\n",
    "mmlu_data = loaders.load_mmlu(sample_size=500)\n",
    "\n",
    "# Executes and saves to /content/drive/MyDrive/spectral_pipeline/mmlu_t1.0_n20.jsonl\n",
    "pipeline.generate_dataset(\n",
    "    data_list=mmlu_data,\n",
    "    answers_per_prompt=20,\n",
    "    dataset_name=\"mmlu\", \n",
    "    temperature=1.0,\n",
    "    overwrite=True # Forces overwrite\n",
    ")\n",
    "\n",
    "print(\"Dataset successfully generated\")\n",
    "\n",
    "\n",
    "# --- Judging mmlu, 20 samples per prompt ---\n",
    "judge = data_generation.LLMJudge(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "target_path = \"/content/drive/MyDrive/spectral_pipeline/mmlu_t1.0_n20.jsonl\"\n",
    "judge.evaluate_file(target_path)\n",
    "\n",
    "# Build dataframe\n",
    "df = pd.read_json(target_path, lines=True)\n",
    "\n",
    "# Columns for inspection\n",
    "display(df[['id', 'sample_num', 'question', 'correctness', 'domain', 'correctness_score'])\n",
    "\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# --- Generate multiple samples per prompt\n",
    "\n",
    "pipeline = data_generation.Pipeline()\n",
    "halueval_data = loaders.load_halueval(sample_size=500)\n",
    "\n",
    "# Executes and saves to /content/drive/MyDrive/spectral_pipeline/halueval_t1.0_n20.jsonl\n",
    "pipeline.generate_dataset(\n",
    "    data_list=halueval_data,\n",
    "    answers_per_prompt=20,\n",
    "    dataset_name=\"halueval\", \n",
    "    temperature=1.0,\n",
    "    overwrite=True \n",
    ")\n",
    "\n",
    "print(\"Dataset successfully generated\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Judging halueval, 20 samples per prompt ---\n",
    "\n",
    "judge = data_generation.LLMJudge(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "target_path = \"/content/drive/MyDrive/spectral_pipeline/halueval_t1.0_n20.jsonl\"\n",
    "judge.evaluate_file(target_path)\n",
    "\n",
    "# Build dataframe\n",
    "df = pd.read_json(target_path, lines=True)\n",
    "\n",
    "# Columns for inspection\n",
    "display(df[['id', 'sample_num', 'question', 'correctness', 'domain', 'correctness_score']])\n",
    "\n",
    "runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbe423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# --- Evaluation for multiple samples (Defan) (Semantic entropy)\n",
    "pipeline = data_generation.Pipeline()\n",
    "\n",
    "# 1. Load the raw JSON file of defan.  Note that this dataset has dimension 500\n",
    "raw_defan = loaders.load_json_file(\"/content/repo/spectral-llm_hallucinations-project/data/raw/sampled_from_defan_500.jsonl\")\n",
    "defan_data = loaders.load_defan(raw_defan)\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "pipeline.generate_dataset(\n",
    "    data_list=defan_data,\n",
    "    answers_per_prompt=20,\n",
    "    dataset_name=\"defan\", \n",
    "    temperature=TEMPERATURE,\n",
    "    overwrite=True # Forces overwrite\n",
    ")\n",
    "\n",
    "print(\"Finalized processing Defan with 20 samples per prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ac056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Judging Defan ---\n",
    "judge = data_generation.LLMJudge(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "target_path = \"/content/drive/MyDrive/spectral_pipeline/defan_t1.0_n20.jsonl\"\n",
    "judge.evaluate_file(target_path)\n",
    "\n",
    "# Build dataframe from storage\n",
    "df = pd.read_json(target_path, lines=True)\n",
    "\n",
    "# Columns for inspection\n",
    "display(df[['id', 'sample_num', 'question', 'correctness', 'domain', 'type', 'correctness_score']])\n",
    "\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
