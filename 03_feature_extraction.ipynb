{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759a2939-24a7-475c-98cd-63634c5e7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "E = np.load(\"response_embeddings.npy\")\n",
    "\n",
    "E_grouped = E.reshape(375,20,384)\n",
    "\n",
    "# write a function that computes and stores the cosine similarity matrices in an np array\n",
    "\n",
    "def compute_cosine_matrices(e):\n",
    "    return e @ e.transpose(0, 2, 1)\n",
    "\n",
    "cosine_mats = compute_cosine_matrices(E_grouped)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f712eb-3174-4e58-982a-b18a3f6f1d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that computes the clusters for the given response ensembles\n",
    "\n",
    "def cluster_gen(e, tau=0.85):\n",
    "    n_questions = e.shape[0]\n",
    "    clusters = []\n",
    "\n",
    "    for i in range(n_questions):\n",
    "        embeddings = e[i]\n",
    "        clustering = AgglomerativeClustering(\n",
    "            metric = \"cosine\",\n",
    "            linkage = \"average\",\n",
    "            distance_threshold = 1-tau,\n",
    "            n_clusters = None\n",
    "        )\n",
    "        labels = clustering.fit_predict(embeddings)\n",
    "        clusters.append(labels)\n",
    "    return clusters\n",
    "\n",
    "# now compute and store the entropy\n",
    "\n",
    "def semantic_entropy(labels):\n",
    "    counts = np.bincount(labels)\n",
    "    probs = counts / counts.sum()\n",
    "    return -(probs * np.log2(probs)).sum()\n",
    "\n",
    "clusters = cluster_gen(E_grouped)\n",
    "H_sem = np.array([semantic_entropy(c) for c in clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9a74d36-40fc-48ab-9cd7-91c5e78edb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the centroid cosine dispersion\n",
    "\n",
    "def centroid_cosine_dispersion(e):\n",
    "    n_questions = e.shape[0]\n",
    "    dispersions = []\n",
    "\n",
    "    for i in range(n_questions):\n",
    "        embeddings = e[i]\n",
    "        centroid = embeddings.mean(axis=0)\n",
    "        centroid = centroid / np.linalg.norm(centroid)\n",
    "        D_cos = np.mean (1 - (embeddings @ centroid))\n",
    "        dispersions.append(D_cos)\n",
    "    return np.array(dispersions)\n",
    "\n",
    "dispersions = centroid_cosine_dispersion(E_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60d387f8-7df3-412c-a28c-2158afe0b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compute mean mahalnobis distance\n",
    "# first fit the reference distribution\n",
    "\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "mask_correct = (df[\"label\"] == 0).to_numpy() \n",
    "\n",
    "E_ref = E[mask_correct]                  \n",
    "\n",
    "mu_ref = E_ref.mean(axis=0)\n",
    "lw = LedoitWolf().fit(E_ref)\n",
    "Sigma_inv = lw.precision_\n",
    "\n",
    "def mean_mahalanobis(e, mu):\n",
    "    diffs = e - mu\n",
    "    qf = np.einsum(\"nkd,dd,nkd->nk\", diffs, Sigma_inv, diffs)\n",
    "    return np.mean(np.sqrt(qf), axis=1)\n",
    "\n",
    "Mbar = mean_mahalanobis(E_grouped, mu_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "281221a2-206e-4997-9c3b-8fd5a6db7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the cluster counts and similarity variance\n",
    "\n",
    "def cluster_count(clusters):\n",
    "    return np.array([len(np.unique(c)) for c in clusters])\n",
    "\n",
    "K = cluster_count(clusters)\n",
    "\n",
    "\n",
    "def similarity_variance(cosine_mats):\n",
    "    n_q = cosine_mats.shape[0]\n",
    "    iu = np.triu_indices(cosine_mats.shape[1], k=1) \n",
    "    out = np.empty(n_q)\n",
    "\n",
    "    for i in range(n_q):\n",
    "        vals = cosine_mats[i][iu]\n",
    "        out[i] = np.var(vals)\n",
    "\n",
    "    return out\n",
    "\n",
    "S_var = similarity_variance(cosine_mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9904c7-074b-49ad-91c8-e7e5425dc6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, construct the feature matrix X and save to file\n",
    "\n",
    "X = np.column_stack([H_sem, dispersions, Mbar, K, S_var])\n",
    "\n",
    "np.save(\"X.npy\", X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
