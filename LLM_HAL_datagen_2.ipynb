{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onDLN98pyQHD"
      },
      "outputs": [],
      "source": [
        "#Colab setup\n",
        "!pip -q install -U transformers accelerate bitsandbytes datasets sentencepiece\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load TruthfulQA\n",
        "\n",
        "ds = load_dataset(\"truthful_qa\", \"generation\")\n",
        "ds"
      ],
      "metadata": {
        "id": "8keG1TV6yZp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"validation\"][0]"
      ],
      "metadata": {
        "id": "W92U3wS1y9Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose split\n",
        "split_name = \"validation\"\n",
        "questions_ds = ds[split_name]\n",
        "print(\"Split size:\", len(questions_ds))"
      ],
      "metadata": {
        "id": "DlxjlCpG4FC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model with eager attention so output_attentions works\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    attn_implementation=\"eager\",   # IMPORTANT for output_attentions=True\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "qbLQAP_A4dzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers, Prompt formatting\n",
        "\n",
        "from transformers.tokenization_utils_base import BatchEncoding\n",
        "\n",
        "def build_prompt(question: str) -> torch.Tensor:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Answer briefly and directly.\"},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        out = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        # Some versions return a tensor; others return BatchEncoding/dict\n",
        "        if isinstance(out, (BatchEncoding, dict)):\n",
        "            input_ids = out[\"input_ids\"]\n",
        "        else:\n",
        "            input_ids = out  # already a tensor\n",
        "    except Exception:\n",
        "        plain_prompt = f\"System: Answer briefly and directly.\\nUser: {question}\\nAssistant:\"\n",
        "        input_ids = tokenizer(plain_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "    return input_ids"
      ],
      "metadata": {
        "id": "DlVy3rwd4eTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top k eigenvalues a laplacian\n",
        "\n",
        "def laplacian_topk_eigs(A: torch.Tensor, k: int, symmetrize=True):\n",
        "    \"\"\"\n",
        "    A: [n, n] attention adjacency (nonnegative)\n",
        "    Returns largest k eigenvalues of unnormalized Laplacian L = D - A.\n",
        "    \"\"\"\n",
        "    if symmetrize:\n",
        "        A = 0.5 * (A + A.T)\n",
        "\n",
        "    A = torch.clamp(A, min=0.0).to(torch.float32)\n",
        "    d = A.sum(dim=1)\n",
        "    L = torch.diag(d) - A\n",
        "\n",
        "    eigs = torch.linalg.eigvalsh(L).real\n",
        "    eigs_sorted = torch.sort(eigs).values  # ascending\n",
        "    k_eff = min(k, eigs_sorted.numel())\n",
        "    topk = eigs_sorted[-k_eff:]            # largest k (ascending within top-k)\n",
        "\n",
        "    # Pad on the left with NaNs if too short\n",
        "    if k_eff < k:\n",
        "        pad = torch.full((k - k_eff,), float(\"nan\"))\n",
        "        topk = torch.cat([pad, topk], dim=0)\n",
        "    return topk  # [k]"
      ],
      "metadata": {
        "id": "EsXJU7T56w0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top k eigenvalues for each attention head\n",
        "\n",
        "def per_head_topk(attentions, prompt_len, total_len, topk=20):\n",
        "    \"\"\"\n",
        "    attentions: tuple length L, each [1,H,S,S]\n",
        "    returns eig tensor [L,H,topk] for generated-token block only.\n",
        "    \"\"\"\n",
        "    gen_idx = slice(prompt_len, total_len)\n",
        "\n",
        "    L = len(attentions)\n",
        "    H = attentions[0].shape[1]\n",
        "\n",
        "    eigs_LHK = torch.empty((L, H, topk), dtype=torch.float32)\n",
        "    for l in range(L):\n",
        "        # [H, gen, gen]\n",
        "        att_l = attentions[l][0, :, gen_idx, gen_idx].detach()\n",
        "        for h in range(H):\n",
        "            eigs_LHK[l, h] = laplacian_topk_eigs(att_l[h], k=topk, symmetrize=True)\n",
        "\n",
        "    return eigs_LHK  # [L,H,topk]"
      ],
      "metadata": {
        "id": "Y_9SEBzN6_6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEMPERATURE = 1.0\n",
        "MAX_NEW_TOKENS = 128\n",
        "TOPK = 20\n",
        "N_QUESTIONS = 25   # set larger later; start small to test\n",
        "\n",
        "# Expected model dims (from your message)\n",
        "Layer = 22\n",
        "Head = 32\n",
        "\n",
        "# Create column names once: eig_l{l}_h{h}_{j}\n",
        "# NOTE: 22*32*20 = 14080 columns (large but fine)\n",
        "def make_feature_columns(L, H, K):\n",
        "    cols = []\n",
        "    for l in range(L):\n",
        "        for h in range(H):\n",
        "            for j in range(K):\n",
        "                cols.append(f\"eig_l{l}_h{h}_{j}\")\n",
        "    return cols\n",
        "\n",
        "feature_cols = make_feature_columns(Layer, Head, TOPK)"
      ],
      "metadata": {
        "id": "w6QDK_-Z6_zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop: build DataFrame rows\n",
        "\n",
        "rows = []\n",
        "failed = 0\n",
        "\n",
        "for idx in range(min(N_QUESTIONS, len(questions_ds))):\n",
        "    ex = questions_ds[idx]\n",
        "    question = ex[\"question\"]\n",
        "\n",
        "    # Build prompt\n",
        "    prompt_ids = build_prompt(question).to(model.device)\n",
        "    prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "    # Generate answer (so we have generated tokens to analyze)\n",
        "    with torch.no_grad():\n",
        "        gen_ids = model.generate(\n",
        "            prompt_ids,\n",
        "            max_new_tokens=MAX_NEW_TOKENS,\n",
        "            temperature=TEMPERATURE,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    total_len = gen_ids.shape[1]\n",
        "    gen_len = total_len - prompt_len\n",
        "\n",
        "    # Skip very short generations (you can choose to keep + pad instead)\n",
        "    if gen_len < 5:\n",
        "        failed += 1\n",
        "        continue\n",
        "\n",
        "    # Forward pass with attentions on the full sequence\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=gen_ids,\n",
        "            output_attentions=True,\n",
        "            use_cache=False,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "    attentions = outputs.attentions\n",
        "    L = len(attentions)\n",
        "    H = attentions[0].shape[1]\n",
        "\n",
        "    # Assert expected shape ((22,32,20))\n",
        "    if (L != Layer) or (H != Head):\n",
        "        raise RuntimeError(f\"Model dims changed: got L={L}, H={H}, expected L={Layer}, H={Head}\")\n",
        "\n",
        "    # Compute [L,H,TOPK]\n",
        "    eigs_LHK = per_head_topk(attentions, prompt_len, total_len, topk=TOPK)\n",
        "\n",
        "    # Flatten to one row (length L*H*TOPK)\n",
        "    feat = eigs_LHK.reshape(-1).cpu().numpy()\n",
        "\n",
        "    row = {\n",
        "        \"row_id\": idx,\n",
        "        \"question\": question,\n",
        "        \"prompt_len\": int(prompt_len),\n",
        "        \"gen_len\": int(gen_len),\n",
        "    }\n",
        "    # Add eigenvalue features\n",
        "    row.update({c: float(v) for c, v in zip(feature_cols, feat)})\n",
        "\n",
        "    rows.append(row)"
      ],
      "metadata": {
        "id": "8-V0kv1i7kJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rows kept:\", len(rows), \"Rows skipped(short):\", failed)"
      ],
      "metadata": {
        "id": "VDBEFiCr8pqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_features = pd.DataFrame(rows)\n",
        "df_features.head()"
      ],
      "metadata": {
        "id": "k4me2_E7HC0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_features.columns)"
      ],
      "metadata": {
        "id": "8LzKabk5HDwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xrhbtlh7HTpO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}