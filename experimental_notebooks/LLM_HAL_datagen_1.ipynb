{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtsFly6esULr"
      },
      "outputs": [],
      "source": [
        "#Colab setup\n",
        "!pip -q install -U transformers accelerate bitsandbytes datasets sentencepiece\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load TruthfulQA\n",
        "\n",
        "ds = load_dataset(\"truthful_qa\", \"generation\")\n",
        "ds"
      ],
      "metadata": {
        "id": "xTBAxzjhuqoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds[\"validation\"][0]"
      ],
      "metadata": {
        "id": "mK0MMpRbUIJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See a few examples\n",
        "questions = ds[\"validation\"][:5][\"question\"]\n",
        "print(questions)"
      ],
      "metadata": {
        "id": "0rG_eKwFvMD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install -U huggingface_hub\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "9XgsUWG1xa3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Llama-Instruct model\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# 4-bit quantization to fit on T4\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             device_map=\"auto\",\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             attn_implementation=\"eager\"\n",
        "                                            )\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "5C7f8dWRvMWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt formatting\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Answer briefly and directly.\"},\n",
        "    {\"role\": \"user\", \"content\": questions},\n",
        "]"
      ],
      "metadata": {
        "id": "xxgJvMg1vkZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "ifuS-Z9dOHAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    prompt_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(\"apply_chat_template failed; falling back to plain prompt.\\n\", e)\n",
        "    plain_prompt = f\"System: Answer briefly and directly.\\nUser: {questions}\\nAssistant:\"\n",
        "    prompt_ids = tokenizer(plain_prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "prompt_ids = prompt_ids.to(model.device)\n",
        "prompt_len = prompt_ids.shape[1]\n",
        "\n",
        "with torch.no_grad():\n",
        "    gen_ids = model.generate(\n",
        "        prompt_ids,\n",
        "        max_new_tokens=128,\n",
        "        temperature=1.0,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "full_text = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Generated (full decoded) ---\\n\", full_text)\n",
        "\n",
        "gen_len = gen_ids.shape[1] - prompt_len\n",
        "print(\"\\nPrompt tokens:\", prompt_len, \"Generated tokens:\", gen_len)"
      ],
      "metadata": {
        "id": "J-TwaFlWzSvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass with output_attentions=True on full sequence\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=gen_ids,\n",
        "        output_attentions=True,\n",
        "        use_cache=False,   # compute full attention tensors\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "attentions = outputs.attentions  # tuple length num_layers\n",
        "num_layers = len(attentions)\n",
        "num_heads = attentions[0].shape[1]\n",
        "seq_len = attentions[0].shape[-1]\n",
        "print(f\"\\nLayers: {num_layers}, Heads: {num_heads}, SeqLen: {seq_len}\")"
      ],
      "metadata": {
        "id": "wc7_UpmY0wU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Laplacian and compute top-k eigenvalues on generated-token subgraph\n",
        "\n",
        "def laplacian_eigs_from_attention(A: torch.Tensor, k=20, symmetrize=True):\n",
        "    \"\"\"\n",
        "    A: [n, n] attention adjacency (nonnegative)\n",
        "    Returns dict k -> smallest k eigenvalues (sorted ascending) of Laplacian L = D - A.\n",
        "    \"\"\"\n",
        "    if symmetrize:\n",
        "        A = 0.5 * (A + A.T)\n",
        "\n",
        "    A = torch.clamp(A, min=0.0)\n",
        "\n",
        "    d = A.sum(dim=1)\n",
        "    L = torch.diag(d) - A\n",
        "\n",
        "    eigs = torch.linalg.eigvalsh(L).real\n",
        "    eigs_sorted = torch.sort(eigs).values\n",
        "\n",
        "    out = {}\n",
        "\n",
        "    k_eff = min(k, eigs_sorted.numel())\n",
        "    out = eigs_sorted[-k_eff:].detach().cpu().numpy()\n",
        "    return out"
      ],
      "metadata": {
        "id": "9nQKYu-T2Em_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_heads(attn_layer: torch.Tensor, mode=\"mean\"):\n",
        "    \"\"\"\n",
        "    attn_layer: [batch, heads, seq, seq] -> [seq, seq]\n",
        "    \"\"\"\n",
        "    A = attn_layer[0]  # [heads, seq, seq]\n",
        "    if mode == \"mean\":\n",
        "        return A.mean(dim=0)\n",
        "    elif mode == \"max\":\n",
        "        return A.max(dim=0).values\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'mean' or 'max'\")"
      ],
      "metadata": {
        "id": "e5zoufDk2m6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_start = prompt_len\n",
        "gen_end = gen_ids.shape[1]\n",
        "gen_idx = slice(gen_start, gen_end)\n",
        "\n",
        "results = {}\n",
        "for layer_idx in range(num_layers):\n",
        "    A_full = aggregate_heads(attentions[layer_idx], mode=\"mean\")      # [seq, seq]\n",
        "    A_gen = A_full[gen_idx, gen_idx].to(torch.float32).cpu()          # [gen_len, gen_len]\n",
        "\n",
        "    if A_gen.shape[0] < 5:\n",
        "        continue\n",
        "\n",
        "    results[layer_idx] = laplacian_eigs_from_attention(A_gen, k=20, symmetrize=True)"
      ],
      "metadata": {
        "id": "i_Wp7u6T2sg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Laplacian eigenvalues on GENERATED-token graph (per layer, head-mean) ---\")\n",
        "for layer_idx in list(results.keys())[:5]:\n",
        "    print(f\"Layer {layer_idx}: {np.round(results[layer_idx], 6)}\")"
      ],
      "metadata": {
        "id": "Pgr1v4-ZRG6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(results.keys()))"
      ],
      "metadata": {
        "id": "skgE02PpNB6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eigenvalues for individual attention heads\n",
        "def laplacian_topk_eigs(A: torch.Tensor, k: int, symmetrize=True):\n",
        "    \"\"\"\n",
        "    A: [n, n] adjacency (attention), nonnegative\n",
        "    Returns largest k eigenvalues of unnormalized Laplacian L = D - A.\n",
        "    \"\"\"\n",
        "    if symmetrize:\n",
        "        A = 0.5 * (A + A.T)\n",
        "\n",
        "    A = torch.clamp(A, min=0.0).to(torch.float32)\n",
        "\n",
        "    d = A.sum(dim=1)\n",
        "    L = torch.diag(d) - A\n",
        "\n",
        "    # symmetric eigvals\n",
        "    eigs = torch.linalg.eigvalsh(L).real\n",
        "    eigs_sorted = torch.sort(eigs).values  # ascending\n",
        "    k_eff = min(k, eigs_sorted.numel())\n",
        "    return eigs_sorted[-k_eff:]            # largest k (still ascending within top-k)"
      ],
      "metadata": {
        "id": "L-yHhs3wNCzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restrict to generated tokens only\n",
        "gen_idx = slice(prompt_len, gen_ids.shape[1])\n",
        "gen_len = gen_ids.shape[1] - prompt_len\n",
        "print(\"gen_len:\", gen_len)\n",
        "\n",
        "L = len(attentions)\n",
        "H = attentions[0].shape[1]\n",
        "print(\"layers:\", L, \"heads:\", H)"
      ],
      "metadata": {
        "id": "ngHly2ChSadK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll store top-20 eigenvalues\n",
        "k = 20\n",
        "eigs_per_head_top20 = torch.empty((L, H, k), dtype=torch.float32)\n",
        "\n",
        "for l in range(L):\n",
        "    # att shape: [1, H, S, S]\n",
        "    att_l = attentions[l][0, :, gen_idx, gen_idx].detach()  # [H, gen, gen]\n",
        "\n",
        "    for h in range(H):\n",
        "        A = att_l[h]  # [gen, gen]\n",
        "        eigs_top = laplacian_topk_eigs(A, k=20, symmetrize=True)\n",
        "\n",
        "        # Pad on the left if sequence is shorter than k (rare unless very short gen)\n",
        "        if eigs_top.numel() < k:\n",
        "            pad = torch.full((k - eigs_top.numel(),), float(\"nan\"))\n",
        "            eigs_top = torch.cat([pad, eigs_top], dim=0)\n",
        "\n",
        "        eigs_per_head_top20[l, h] = eigs_top"
      ],
      "metadata": {
        "id": "wo5UjSy-SqY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigs_per_head_top20.shape"
      ],
      "metadata": {
        "id": "aoVNVzkhSwdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eigs_per_head_top20[0,0]"
      ],
      "metadata": {
        "id": "v6X_qRmFS59a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsVTJYC5Y-_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}