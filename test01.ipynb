{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "671daba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip uninstall -y datasets\n",
    "# !python -m pip install -U datasets pyarrow huggingface-hub fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925350dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading libraries...\n",
      "✓ datasets imported successfully\n",
      "✓ transformers imported successfully\n",
      "✓ sentence_transformers imported successfully\n",
      "✓ sklearn, scipy, xgboost imported successfully\n",
      "✓ torch imported successfully\n",
      "✓ tqdm imported successfully\n",
      "\n",
      "All imports successful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LLM Hallucination Detection via Semantic Entropy & Embedding Geometry\n",
    "**Multi-Subject Analysis**: Compare hallucination detection across multiple MMLU subjects\n",
    "\n",
    "Based on: Farquhar et al. (2024) · Ricco et al. (2025) · Lee et al. (2018)\n",
    "Dataset: MMLU (Massive Multitask Language Understanding)\n",
    "Runtime: Google Colab (free tier, T4 GPU recommended)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# Clear any partially imported datasets module\n",
    "if 'datasets' in sys.modules:\n",
    "    del sys.modules['datasets']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Loading libraries...\")\n",
    "\n",
    "# Import with proper error handling\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    print(\"✓ datasets imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ datasets import error: {e}\")\n",
    "    print(\"  Attempting to reload...\")\n",
    "    importlib.reload(sys.modules.get('datasets', __import__('datasets')))\n",
    "    from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "    print(\"✓ transformers imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ transformers import failed: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"✓ sentence_transformers imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ sentence_transformers import failed: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    from scipy.stats import ks_2samp\n",
    "    import xgboost as xgb\n",
    "    print(\"✓ sklearn, scipy, xgboost imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ sklearn/scipy/xgboost import failed: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"✓ torch imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ torch import failed: {e}\")\n",
    "    raise\n",
    "\n",
    "import time\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"✓ tqdm imported successfully\")\n",
    "except:\n",
    "    print(\"⚠ tqdm not available, using simple progress\")\n",
    "    def tqdm(iterable, desc=None):\n",
    "        return iterable\n",
    "\n",
    "print(\"\\nAll imports successful\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e90f7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n",
      "Subjects to analyze: 5 subjects\n",
      "Questions per subject: 50\n",
      "Samples per question: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset_name\"   : \"cais/mmlu\",\n",
    "    \"mmlu_subjects\"  : [\n",
    "        \"high_school_geography\",\n",
    "        \"high_school_world_history\",\n",
    "        \"high_school_physics\",\n",
    "        \"high_school_biology\",\n",
    "        \"high_school_us_history\",\n",
    "    ],  # ← Add/remove subjects here\n",
    "    \"n_questions\"    : 50,         # REDUCED for faster testing (increase to 100-200 for production)\n",
    "    \"n_samples\"      : 20,         # Responses per question\n",
    "    \"temperature\"    : 0.5,        # Higher = more diversity\n",
    "    \"sim_threshold\"  : 0.85,       # Cosine similarity threshold\n",
    "    \"generator_model\": \"google/flan-t5-base\",\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"random_seed\"    : 42,\n",
    "}\n",
    "\n",
    "np.random.seed(CONFIG[\"random_seed\"])\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ANSWER_MAP = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "\n",
    "print(f\"Running on: {DEVICE}\")\n",
    "print(f\"Subjects to analyze: {len(CONFIG['mmlu_subjects'])} subjects\")\n",
    "print(f\"Questions per subject: {CONFIG['n_questions']}\")\n",
    "print(f\"Samples per question: {CONFIG['n_samples']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc632025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models (this may take 1-2 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246374c4cf864ffdbcdc8be5be2ec6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generator loaded in 1.5s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5589df1b0845c49bb1776d17f462ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embedder loaded in 3.5s\n",
      "✓ All models loaded in 3.5s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# LOAD MODELS (once)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading models (this may take 1-2 minutes)...\")\n",
    "start_model_load = time.time()\n",
    "\n",
    "try:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(CONFIG[\"generator_model\"])\n",
    "    generator = T5ForConditionalGeneration.from_pretrained(CONFIG[\"generator_model\"]).to(DEVICE)\n",
    "    generator.eval()\n",
    "    print(f\"✓ Generator loaded in {time.time() - start_model_load:.1f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load generator: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    embedder = SentenceTransformer(CONFIG[\"embedding_model\"])\n",
    "    print(f\"✓ Embedder loaded in {time.time() - start_model_load:.1f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load embedder: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"✓ All models loaded in {time.time() - start_model_load:.1f}s\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3af95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def build_prompt(item):\n",
    "    choices = item[\"choices\"]\n",
    "    return (\n",
    "        f\"Answer the following multiple choice question. \"\n",
    "        f\"Respond with only the letter A, B, C, or D.\\n\\n\"\n",
    "        f\"Question: {item['question']}\\n\"\n",
    "        f\"A) {choices[0]}\\nB) {choices[1]}\\nC) {choices[2]}\\nD) {choices[3]}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "def sample_responses(item, n_samples, temperature, device=DEVICE):\n",
    "    \"\"\"Generate multiple responses with nucleus sampling\"\"\"\n",
    "    prompt = build_prompt(item)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = generator.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=n_samples,\n",
    "        )\n",
    "    \n",
    "    responses = [tokenizer.decode(o, skip_special_tokens=True).strip() for o in outputs]\n",
    "    return responses\n",
    "\n",
    "def extract_answer_letter(response_text):\n",
    "    \"\"\"Extract A/B/C/D from response\"\"\"\n",
    "    for char in response_text.upper().strip():\n",
    "        if char in (\"A\", \"B\", \"C\", \"D\"):\n",
    "            return char\n",
    "    return None\n",
    "\n",
    "def label_responses(item, responses):\n",
    "    \"\"\"Label responses as correct (0) or hallucinated (1)\"\"\"\n",
    "    correct = ANSWER_MAP[item[\"answer\"]]\n",
    "    labels = [0 if extract_answer_letter(r) == correct else 1 for r in responses]\n",
    "    hall_rate = np.mean(labels)\n",
    "    return labels, hall_rate, int(hall_rate > 0.5)\n",
    "\n",
    "def semantic_entropy(embs, threshold=CONFIG[\"sim_threshold\"]):\n",
    "    \"\"\"Compute semantic entropy from embeddings\"\"\"\n",
    "    N = len(embs)\n",
    "    dist_matrix = np.clip(1 - cosine_similarity(embs), 0, None)\n",
    "    np.fill_diagonal(dist_matrix, 0)\n",
    "    \n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None, metric=\"precomputed\",\n",
    "        linkage=\"average\", distance_threshold=1 - threshold\n",
    "    )\n",
    "    ids = clustering.fit_predict(dist_matrix)\n",
    "    _, counts = np.unique(ids, return_counts=True)\n",
    "    probs = counts / N\n",
    "    H = -np.sum(probs * np.log2(probs + 1e-12))\n",
    "    return H, len(counts)\n",
    "\n",
    "def cosine_dispersion(embs):\n",
    "    \"\"\"Compute cosine dispersion\"\"\"\n",
    "    centroid = embs.mean(axis=0, keepdims=True)\n",
    "    return np.mean(1 - cosine_similarity(embs, centroid).flatten())\n",
    "\n",
    "def mahalanobis_distance(embs, mu, cov_inv):\n",
    "    \"\"\"Compute Mahalanobis distance\"\"\"\n",
    "    diffs = embs - mu\n",
    "    return np.mean(np.sqrt(np.einsum(\"ni,ij,nj->n\", diffs, cov_inv, diffs)))\n",
    "\n",
    "def similarity_variance(embs):\n",
    "    \"\"\"Compute variance of pairwise similarities\"\"\"\n",
    "    sim = cosine_similarity(embs)\n",
    "    upper = sim[np.triu_indices(len(embs), k=1)]\n",
    "    return np.var(upper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10c6f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING MULTI-SUBJECT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "[1/5] high_school_geography\n",
      "======================================================================\n",
      "Loading dataset... ✓ (50 questions)\n",
      "Sampling 20 responses per question... ✓\n",
      "Labeling responses... ✓\n",
      "  Correct: 20 (40.0%) | Hallucinated: 30 (60.0%)\n",
      "Embedding 1000 responses... ✓\n",
      "Fitting Mahalanobis reference... ✓ (395 correct responses)\n",
      "Extracting features... ✓\n",
      "Running statistical tests... ✓\n",
      "Running classification (5-fold CV)... ✓ (AUC=0.435)\n",
      "Computing bootstrap CI... ✓\n",
      " Completed in 82.7s\n",
      "\n",
      "======================================================================\n",
      "[2/5] high_school_world_history\n",
      "======================================================================\n",
      "Loading dataset... \n",
      "  ✗ Failed to load high_school_world_history: RLock objects should only be shared between processes through inheritance\n",
      "\n",
      "======================================================================\n",
      "[3/5] high_school_physics\n",
      "======================================================================\n",
      "Loading dataset... \n",
      "  ✗ Failed to load high_school_physics: RLock objects should only be shared between processes through inheritance\n",
      "\n",
      "======================================================================\n",
      "[4/5] high_school_biology\n",
      "======================================================================\n",
      "Loading dataset... \n",
      "  ✗ Failed to load high_school_biology: RLock objects should only be shared between processes through inheritance\n",
      "\n",
      "======================================================================\n",
      "[5/5] high_school_us_history\n",
      "======================================================================\n",
      "Loading dataset... \n",
      "  ✗ Failed to load high_school_us_history: RLock objects should only be shared between processes through inheritance\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE: PROCESS ALL SUBJECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING MULTI-SUBJECT ANALYSIS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "results_all_subjects = {}\n",
    "timing_log = []\n",
    "\n",
    "for subject_idx, subject in enumerate(CONFIG[\"mmlu_subjects\"], 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{subject_idx}/{len(CONFIG['mmlu_subjects'])}] {subject}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load dataset\n",
    "    try:\n",
    "        print(f\"Loading dataset...\", end=\" \", flush=True)\n",
    "        dataset = load_dataset(CONFIG[\"dataset_name\"], subject, split=\"test\")\n",
    "        n = min(CONFIG[\"n_questions\"], len(dataset))\n",
    "        indices = np.random.choice(len(dataset), n, replace=False)\n",
    "        questions_raw = [dataset[int(i)] for i in indices]\n",
    "        print(f\"✓ ({n} questions)\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ✗ Failed to load {subject}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Sample responses\n",
    "    print(f\"Sampling {CONFIG['n_samples']} responses per question...\", end=\" \", flush=True)\n",
    "    all_responses = []\n",
    "    for item in questions_raw:\n",
    "        responses = sample_responses(item, CONFIG[\"n_samples\"], CONFIG[\"temperature\"])\n",
    "        all_responses.append(responses)\n",
    "    print(f\"✓\")\n",
    "    \n",
    "    # Label responses\n",
    "    print(f\"Labeling responses...\", end=\" \", flush=True)\n",
    "    response_labels = []\n",
    "    hallucination_rates = []\n",
    "    question_labels = []\n",
    "    \n",
    "    for item, responses in zip(questions_raw, all_responses):\n",
    "        labels, hall_rate, q_label = label_responses(item, responses)\n",
    "        response_labels.append(labels)\n",
    "        hallucination_rates.append(hall_rate)\n",
    "        question_labels.append(q_label)\n",
    "    \n",
    "    y = np.array(question_labels)\n",
    "    print(f\"✓\")\n",
    "    \n",
    "    print(f\"  Correct: {(y==0).sum()} ({(y==0).mean()*100:.1f}%) | \"\n",
    "          f\"Hallucinated: {(y==1).sum()} ({(y==1).mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Embed responses\n",
    "    print(f\"Embedding {len(all_responses) * CONFIG['n_samples']} responses...\", end=\" \", flush=True)\n",
    "    all_embeddings = []\n",
    "    for responses in all_responses:\n",
    "        embs = embedder.encode(responses, normalize_embeddings=True, show_progress_bar=False)\n",
    "        all_embeddings.append(embs)\n",
    "    print(f\"✓\")\n",
    "    \n",
    "    # Fit Mahalanobis reference\n",
    "    print(f\"Fitting Mahalanobis reference...\", end=\" \", flush=True)\n",
    "    correct_embs = np.array([\n",
    "        emb for embs, labels in zip(all_embeddings, response_labels)\n",
    "        for emb, lbl in zip(embs, labels) if lbl == 0\n",
    "    ])\n",
    "    \n",
    "    if len(correct_embs) > 1:\n",
    "        mu_ref = correct_embs.mean(axis=0)\n",
    "        cov_ref = np.cov(correct_embs.T) + np.eye(correct_embs.shape[1]) * 1e-6\n",
    "        cov_inv = np.linalg.pinv(cov_ref)\n",
    "        print(f\"✓ ({len(correct_embs)} correct responses)\")\n",
    "    else:\n",
    "        print(f\"✗ Not enough correct responses\")\n",
    "        continue\n",
    "    \n",
    "    # Extract features\n",
    "    print(f\"Extracting features...\", end=\" \", flush=True)\n",
    "    features = []\n",
    "    for embs in all_embeddings:\n",
    "        H, K = semantic_entropy(embs)\n",
    "        D = cosine_dispersion(embs)\n",
    "        M = mahalanobis_distance(embs, mu_ref, cov_inv)\n",
    "        sig2 = similarity_variance(embs)\n",
    "        features.append([H, D, M, K, sig2])\n",
    "    print(f\"✓\")\n",
    "    \n",
    "    X = np.array(features)\n",
    "    feature_names = [\"H_sem\", \"D_cos\", \"M_bar\", \"K\", \"sig2_S\"]\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df[\"label\"] = y\n",
    "    df[\"subject\"] = subject\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(f\"Running statistical tests...\", end=\" \", flush=True)\n",
    "    alpha_bonf = 0.05 / len(feature_names)\n",
    "    ks_results = {}\n",
    "    for col in feature_names:\n",
    "        g0 = df[df.label==0][col]\n",
    "        g1 = df[df.label==1][col]\n",
    "        if len(g0) > 1 and len(g1) > 1:\n",
    "            stat, p = ks_2samp(g0, g1)\n",
    "            ks_results[col] = {\"stat\": stat, \"p\": p, \"significant\": p < alpha_bonf}\n",
    "        else:\n",
    "            ks_results[col] = {\"stat\": 0, \"p\": 1.0, \"significant\": False}\n",
    "    print(f\"✓\")\n",
    "    \n",
    "    # Classification\n",
    "    print(f\"Running classification (5-fold CV)...\", end=\" \", flush=True)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=min(5, len(np.unique(y))), shuffle=True, random_state=CONFIG[\"random_seed\"])\n",
    "    \n",
    "    best_clf = RandomForestClassifier(n_estimators=100, random_state=CONFIG[\"random_seed\"], n_jobs=-1)\n",
    "    try:\n",
    "        scores = cross_val_score(best_clf, X_scaled, y, cv=cv, scoring=\"roc_auc\")\n",
    "        auc_cv_mean = scores.mean()\n",
    "        auc_cv_std = scores.std()\n",
    "    except:\n",
    "        auc_cv_mean = 0.5\n",
    "        auc_cv_std = 0.0\n",
    "    \n",
    "    print(f\"✓ (AUC={auc_cv_mean:.3f})\")\n",
    "    \n",
    "    # Bootstrap CI (simplified - 500 iterations instead of 1000)\n",
    "    print(f\"Computing bootstrap CI...\", end=\" \", flush=True)\n",
    "    auc_boot = []\n",
    "    rng = np.random.default_rng(CONFIG[\"random_seed\"])\n",
    "    for _ in range(500):  # Reduced from 1000 for speed\n",
    "        idx = rng.choice(len(y), len(y), replace=True)\n",
    "        oob = np.setdiff1d(np.arange(len(y)), idx)\n",
    "        if len(np.unique(y[oob])) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            clf_b = RandomForestClassifier(n_estimators=50, random_state=0, n_jobs=-1)\n",
    "            clf_b.fit(X_scaled[idx], y[idx])\n",
    "            proba = clf_b.predict_proba(X_scaled[oob])[:, 1]\n",
    "            auc_boot.append(roc_auc_score(y[oob], proba))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if auc_boot:\n",
    "        ci_lo, ci_hi = np.percentile(auc_boot, [2.5, 97.5])\n",
    "        auc_mean = np.mean(auc_boot)\n",
    "    else:\n",
    "        ci_lo, ci_hi = auc_cv_mean - 0.1, auc_cv_mean + 0.1\n",
    "        auc_mean = auc_cv_mean\n",
    "    \n",
    "    print(f\"✓\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    timing_log.append({\"subject\": subject, \"seconds\": elapsed})\n",
    "    \n",
    "    # Store results\n",
    "    results_all_subjects[subject] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"df\": df,\n",
    "        \"scaler\": scaler,\n",
    "        \"X_scaled\": X_scaled,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"n_questions\": n,\n",
    "        \"hallucination_rate\": np.mean(hallucination_rates),\n",
    "        \"correct_count\": (y==0).sum(),\n",
    "        \"hallucinated_count\": (y==1).sum(),\n",
    "        \"auc_mean\": auc_mean,\n",
    "        \"auc_ci_lo\": ci_lo,\n",
    "        \"auc_ci_hi\": ci_hi,\n",
    "        \"auc_cv_mean\": auc_cv_mean,\n",
    "        \"auc_cv_std\": auc_cv_std,\n",
    "        \"ks_results\": ks_results,\n",
    "        \"correct_embs\": correct_embs,\n",
    "        \"mu_ref\": mu_ref,\n",
    "        \"cov_inv\": cov_inv,\n",
    "    }\n",
    "    \n",
    "    print(f\" Completed in {elapsed:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CROSS-SUBJECT SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "if results_all_subjects:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"CROSS-SUBJECT SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    summary_data = []\n",
    "    for subject, res in results_all_subjects.items():\n",
    "        summary_data.append({\n",
    "            \"Subject\": subject.replace(\"high_school_\", \"\"),\n",
    "            \"N\": res[\"n_questions\"],\n",
    "            \"Hall.%\": f\"{res['hallucination_rate']*100:.1f}\",\n",
    "            \"AUC\": f\"{res['auc_mean']:.3f}\",\n",
    "            \"95% CI\": f\"[{res['auc_ci_lo']:.3f}, {res['auc_ci_hi']:.3f}]\",\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n Analysis complete! {len(results_all_subjects)} subjects processed\")\n",
    "    print(f\"Total runtime: {sum(t['seconds'] for t in timing_log):.1f}s\")\n",
    "else:\n",
    "    print(\"\\n✗ No subjects processed successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS OBJECT FOR SUPPLEMENTARY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n Results saved to 'results_all_subjects' dictionary\")\n",
    "print(\"   Run supplementary_analysis.py next to generate additional plots and tables\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_ds_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
